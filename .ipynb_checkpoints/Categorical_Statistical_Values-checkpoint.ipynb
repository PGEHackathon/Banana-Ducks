{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df92b015",
   "metadata": {},
   "source": [
    "# Categorical Data - Statistical Analysis\n",
    "\n",
    "This notebook uses Scikit-Learn to handle missing data, analyze, and visualize a dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abebfb6",
   "metadata": {},
   "source": [
    "## Importing Libraries\n",
    "\n",
    "Importing required libraries:\n",
    "- `pandas`: For data handling.\n",
    "- `matplotlib.pyplot` and `seaborn`: For plotting.\n",
    "- `SimpleImputer`: For filling missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4da0a509",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os                                                 # to set current working directory \n",
    "import math                                               # basic calculations like square root\n",
    "from sklearn.model_selection import train_test_split      # train and test split\n",
    "from sklearn import svm                                   # support vector machine methods\n",
    "from sklearn import tree                                  # tree program from scikit learn (package for machine learning)\n",
    "from sklearn.metrics import mean_squared_error, r2_score  # specific measures to check our models\n",
    "from sklearn.impute import SimpleImputer\n",
    "import pandas as pd                                       # DataFrames and plotting\n",
    "import pandas.plotting as pd_plot\n",
    "import numpy as np                                        # arrays and matrix math\n",
    "import matplotlib.pyplot as plt                           # plotting\n",
    "from intake import cat                                    # data catalogue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011ceb6d",
   "metadata": {},
   "source": [
    "#### Declare functions\n",
    "\n",
    "Let's define a couple of functions to streamline plotting correlation matrices and visualization of a machine learning regression model responce over the 2 predictor features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48ead0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_corr(dataframe,size=10):                         # plots a graphical correlation matrix \n",
    "    corr = dataframe.corr()\n",
    "    fig, ax = plt.subplots(figsize=(size, size))\n",
    "    im = ax.matshow(corr,vmin = -1.0, vmax = 1.0)\n",
    "    plt.xticks(range(len(corr.columns)), corr.columns);\n",
    "    plt.yticks(range(len(corr.columns)), corr.columns);\n",
    "    plt.colorbar(im, orientation = 'vertical')\n",
    "    plt.title('Correlation Matrix')\n",
    "    \n",
    "def visualize_model(model,xfeature,yfeature,response,title,):# plots the data points and the decision tree prediction \n",
    "    n_classes = 10\n",
    "    cmap = plt.cm.RdYlBu\n",
    "    plot_step = 0.02\n",
    "    x_min, x_max = min(xfeature) - 1, max(xfeature) + 1\n",
    "    y_min, y_max = min(yfeature) - 1, max(yfeature) + 1\n",
    "    resp_min = round(min(response)); resp_max = round(max(response));\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),\n",
    "                     np.arange(y_min, y_max, plot_step))\n",
    "    z_min = round(min(response)); z_max = round(max(response))\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    cs = plt.contourf(xx, yy, Z, cmap=cmap,vmin=z_min, vmax=z_max)\n",
    "\n",
    "    im = plt.scatter(xfeature,yfeature,s=None, c=response, marker=None, cmap=cmap, norm=None, vmin=z_min, vmax=z_max, alpha=0.8, linewidths=0.3, edgecolors=\"black\")\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xfeature.name)\n",
    "    plt.ylabel(yfeature.name)\n",
    "    cbar = plt.colorbar(im, orientation = 'vertical')\n",
    "    cbar.set_label(response.name, rotation=270, labelpad=20)\n",
    "    return(plt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf377b75",
   "metadata": {},
   "source": [
    "#### Read the data table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c124a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data = pd.read_csv(\"Categorical_HackathonData2024.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244a6e7e",
   "metadata": {},
   "source": [
    "## Statistical Analysis\n",
    "\n",
    "Calculate basic statistics (mean, standard deviation, etc.) for the numeric data.\n",
    "\n",
    "Let's visualize the first several rows of our data stored in a DataFrame so we can make sure we successfully loaded the data file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71481de6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Well ID</th>\n",
       "      <th>Avg Pump Difference</th>\n",
       "      <th>Area</th>\n",
       "      <th>Fluid System</th>\n",
       "      <th>Development Strategy</th>\n",
       "      <th>DELAYED</th>\n",
       "      <th>BOUND_CODE</th>\n",
       "      <th>CODEV_POSITION</th>\n",
       "      <th>CODEV_FORMATION_POSITION</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.93</td>\n",
       "      <td>Acadia</td>\n",
       "      <td>Campbell</td>\n",
       "      <td>Coke</td>\n",
       "      <td>1</td>\n",
       "      <td>11100000</td>\n",
       "      <td>Edge</td>\n",
       "      <td>Edge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>-8.45</td>\n",
       "      <td>Acadia</td>\n",
       "      <td>Campbell</td>\n",
       "      <td>Coke</td>\n",
       "      <td>1</td>\n",
       "      <td>11100000</td>\n",
       "      <td>Edge</td>\n",
       "      <td>Edge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>8.70</td>\n",
       "      <td>Arches</td>\n",
       "      <td>Campbell</td>\n",
       "      <td>Orange Crush</td>\n",
       "      <td>0</td>\n",
       "      <td>101111</td>\n",
       "      <td>Middle</td>\n",
       "      <td>Edge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>19.93</td>\n",
       "      <td>Badlands</td>\n",
       "      <td>Campbell</td>\n",
       "      <td>Mountain Dew</td>\n",
       "      <td>1</td>\n",
       "      <td>10001101</td>\n",
       "      <td>Edge</td>\n",
       "      <td>Edge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>-4.34</td>\n",
       "      <td>Big Bend</td>\n",
       "      <td>Campbell</td>\n",
       "      <td>Coke</td>\n",
       "      <td>1</td>\n",
       "      <td>11110000</td>\n",
       "      <td>Edge</td>\n",
       "      <td>Edge</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Well ID  Avg Pump Difference      Area Fluid System Development Strategy  \\\n",
       "0        1                -0.93    Acadia     Campbell                 Coke   \n",
       "1        2                -8.45    Acadia     Campbell                 Coke   \n",
       "2        3                 8.70    Arches     Campbell         Orange Crush   \n",
       "3        5                19.93  Badlands     Campbell         Mountain Dew   \n",
       "4        6                -4.34  Big Bend     Campbell                 Coke   \n",
       "\n",
       "   DELAYED  BOUND_CODE CODEV_POSITION CODEV_FORMATION_POSITION  \n",
       "0        1    11100000           Edge                     Edge  \n",
       "1        1    11100000           Edge                     Edge  \n",
       "2        0      101111         Middle                     Edge  \n",
       "3        1    10001101           Edge                     Edge  \n",
       "4        1    11110000           Edge                     Edge  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_data.head()                                            # preview the first 5 rows of the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169885a6",
   "metadata": {},
   "source": [
    "Let's remove the well index and check the summary summary statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b66bf28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Avg Pump Difference</th>\n",
       "      <td>333.0</td>\n",
       "      <td>2.246405e+01</td>\n",
       "      <td>2.432724e+01</td>\n",
       "      <td>-12.69</td>\n",
       "      <td>3.86</td>\n",
       "      <td>17.37</td>\n",
       "      <td>34.33</td>\n",
       "      <td>117.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DELAYED</th>\n",
       "      <td>334.0</td>\n",
       "      <td>7.155689e-01</td>\n",
       "      <td>4.518199e-01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BOUND_CODE</th>\n",
       "      <td>334.0</td>\n",
       "      <td>9.898158e+06</td>\n",
       "      <td>2.817472e+06</td>\n",
       "      <td>1110.00</td>\n",
       "      <td>10001001.00</td>\n",
       "      <td>11011001.00</td>\n",
       "      <td>11111000.00</td>\n",
       "      <td>11111111.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     count          mean           std      min          25%  \\\n",
       "Avg Pump Difference  333.0  2.246405e+01  2.432724e+01   -12.69         3.86   \n",
       "DELAYED              334.0  7.155689e-01  4.518199e-01     0.00         0.00   \n",
       "BOUND_CODE           334.0  9.898158e+06  2.817472e+06  1110.00  10001001.00   \n",
       "\n",
       "                             50%          75%          max  \n",
       "Avg Pump Difference        17.37        34.33       117.26  \n",
       "DELAYED                     1.00         1.00         1.00  \n",
       "BOUND_CODE           11011001.00  11111000.00  11111111.00  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_data = my_data.iloc[:,1:34]                             # copy all rows and columns 1 through 8, note 0 column is removed\n",
    "my_data.describe().transpose()                            # calculate summary statistics for the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de17d7c6",
   "metadata": {},
   "source": [
    "Some values may be negative that cannot be negative so we can truncate these, we will leave the values as is for the time being."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af107aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#num = my_data._get_numeric_data()                         # get the numerical values\n",
    "#num[num < 0] = 0                                          # truncate negative values to 0.0\n",
    "#my_data.describe().transpose()                            # calculate summary statistics for the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd638244",
   "metadata": {},
   "source": [
    "Let's assign min and max values for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2ac6cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fmin = [-15,2200,7250,0,250,125,140]\n",
    "fmax = [125,18500,11000,10,3000,3000,3000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b75a09",
   "metadata": {},
   "source": [
    "#### Calculate the correlation matrix \n",
    "\n",
    "For multivariate analysis it is a good idea to check the correlation matrix.  We can calculate it and view it in the console with these commands.\n",
    "\n",
    "##### Handling Missing Values\n",
    "\n",
    "Missing values are filled in using the mean to allow creation of the correlation matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06a50531",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot use mean strategy with non-numeric data:\ncould not convert string to float: 'Acadia'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_39928\\3955921304.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mimputer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSimpleImputer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'mean'\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# or another appropriate strategy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmy_data_imputed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimputer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmy_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmy_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mcorr_matrix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorrcoef\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmy_data_imputed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrowvar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorr_matrix\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m                           \u001b[1;31m# print the correlation matrix to 2 decimals\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\_set_output.py\u001b[0m in \u001b[0;36mwrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    140\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 142\u001b[1;33m         \u001b[0mdata_to_wrap\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    143\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_to_wrap\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m             \u001b[1;31m# only wrap the first output for cross decomposition\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\_set_output.py\u001b[0m in \u001b[0;36mwrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    140\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 142\u001b[1;33m         \u001b[0mdata_to_wrap\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    143\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_to_wrap\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m             \u001b[1;31m# only wrap the first output for cross decomposition\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\_set_output.py\u001b[0m in \u001b[0;36mwrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    140\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 142\u001b[1;33m         \u001b[0mdata_to_wrap\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    143\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_to_wrap\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m             \u001b[1;31m# only wrap the first output for cross decomposition\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    846\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    847\u001b[0m             \u001b[1;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 848\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    849\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    850\u001b[0m             \u001b[1;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\impute\\_base.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    387\u001b[0m             )\n\u001b[0;32m    388\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 389\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_input\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0min_fit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    390\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    391\u001b[0m         \u001b[1;31m# default fill_value is 0 for numerical input and \"missing_value\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\impute\\_base.py\u001b[0m in \u001b[0;36m_validate_input\u001b[1;34m(self, X, in_fit)\u001b[0m\n\u001b[0;32m    339\u001b[0m                     )\n\u001b[0;32m    340\u001b[0m                 )\n\u001b[1;32m--> 341\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mnew_ve\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    342\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    343\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mve\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot use mean strategy with non-numeric data:\ncould not convert string to float: 'Acadia'"
     ]
    }
   ],
   "source": [
    "imputer = SimpleImputer(strategy='mean')  # or another appropriate strategy\n",
    "my_data_imputed = pd.DataFrame(imputer.fit_transform(my_data), columns=my_data.columns)\n",
    "\n",
    "corr_matrix = np.corrcoef(my_data_imputed, rowvar = False)\n",
    "print(np.around(corr_matrix,2))                           # print the correlation matrix to 2 decimals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c69711a",
   "metadata": {},
   "source": [
    "Note the 1.0 diagonal resulting from the correlation of each variable with themselves.  \n",
    "\n",
    "Let's use our function declared above to make a graphical correlation matrix visualization.  This may inprove our ability to spot features.  It relies on the built in correlation matrix method with Numpy DataFrames and MatPlotLib for plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027a2710",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_corr(my_data,10)                                      # using our correlation matrix visualization function\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4bc3c88",
   "metadata": {},
   "source": [
    "This looks good.  There is a mix of correlation magnitudes. Of course, correlation coeffficients are limited to degree of linear correlations.  For more complete information, let's look at the matrix scatter plot from the Pandas package. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8069da4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_plot.scatter_matrix(my_data, alpha = 0.1,              # pandas matrix scatter plot\n",
    "    figsize=(10, 10),color = 'black', hist_kwds={'color':['grey']})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28718f9",
   "metadata": {},
   "source": [
    "Visualize the distribution of each column of data using a histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1b7ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_histograms(dataframe, bins=5):\n",
    "    \"\"\"\n",
    "    Plots histograms for each column in the given dataframe.\n",
    "    \n",
    "    Args:\n",
    "    dataframe (pd.DataFrame): The dataframe containing the data.\n",
    "    bins (int): Number of bins for the histograms. Default is 5.\n",
    "    \"\"\"\n",
    "    for column in dataframe.columns:\n",
    "        plt.figure()\n",
    "        dataframe[column].hist(bins=bins)\n",
    "        plt.title(f'Histogram of {column}')\n",
    "        plt.xlabel(column)\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.show()\n",
    "\n",
    "# Call the function with your data\n",
    "plot_histograms(my_data, bins=50)  # Change 'bins' to adjust the number of bins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d8adbf",
   "metadata": {},
   "source": [
    "#### Working with Only Two Predictor Features to Predict One Response Feature - EVERYTHING BELOW HERE IS INCOMPLETE\n",
    "\n",
    "Let's simplify the problem to 2 features, to predict one response feature, Production rate.  We will also reduce the number of wells from 1,000 to 500.  By working with only 2 predictor features, it is very easy to visualize the segmentation of the feature space (it is only 2D and the model can be interogated exhaustively on a single plot).\n",
    "\n",
    "We will demonstrate with:\n",
    "\n",
    "* Porosity and Brittleness\n",
    "\n",
    "to predict:\n",
    "\n",
    "* Production\n",
    "\n",
    "Then you will have a chance to run the workflow again with your own selection for predictor features.\n",
    "\n",
    "To update the predictor features change the code in the box below:\n",
    "\n",
    "```python\n",
    "predictor_features = ['Por','Brittle'] \n",
    "```\n",
    "\n",
    "You can any of these:\n",
    "\n",
    "1. Por\n",
    "2. LogPerm\n",
    "3. AI\n",
    "4. Brittle\n",
    "5. TOC\n",
    "6. VR\n",
    "\n",
    "Note the feature names are caps sensitive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c05663",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select 2 predictor features here:\n",
    "predictor_features = ['Avg Open Pressure','Avg Close Pressure']           # for the first demonstration run we will use porosity and brittleness\n",
    "\n",
    "response_feature = 'Avg Pump Difference'\n",
    "\n",
    "pindex = np.argwhere(my_data.columns.isin(predictor_features)).ravel()\n",
    "print('Selected predictor features: \\n0: ' + predictor_features[0] + ', index = ' + str(pindex[0]) + '.')\n",
    "print('1: ' + predictor_features[1] + ', index = ' + str(pindex[1]) + '.')\n",
    "rindex = np.argwhere(my_data.columns.isin([response_feature])).ravel()\n",
    "print('\\nSelected response feature: \\n' + response_feature + ', index = ' + str(rindex[0]) + '.')\n",
    "\n",
    "#my_data_cut = my_data.iloc[0:500,:] \n",
    "X = my_data[predictor_features]                # extract the 2 selected response features, 500 samples to a X array\n",
    "y = my_data[[response_feature]]                # extract selected response 500 samples to a response array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b1cff2",
   "metadata": {},
   "source": [
    "Let's check the summary statistics of Predictor Features 1 and 2 and the Response Feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c18152a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.describe().transpose()                   # calculate summary statistics for the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58fe5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.describe().transpose()                   # calculate summary statistics for the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa11f25a",
   "metadata": {},
   "source": [
    "Now let's withhold 100 samples as testing data and retain the remaining 400 as training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0063431f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=73073)\n",
    "n_train = len(X_train)\n",
    "n_test = len(X_test)\n",
    "print('Number of training ' + str(n_train) + ', number of test ' + str(n_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837b67fe",
   "metadata": {},
   "source": [
    "Let's compare the univariate statistics of Predictor Features 1 and 2 and the Response Feature for the training and testing datasets.  \n",
    " \n",
    " * let's check for bias and extrapolation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f466b19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.describe().transpose()                   # calculate summary statistics for the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0621174d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.describe().transpose()                   # calculate summary statistics for the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e560a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.describe().transpose()                   # calculate summary statistics for the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e147b73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.describe().transpose()                   # calculate summary statistics for the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e262a7d",
   "metadata": {},
   "source": [
    "Now let's plot the training and testing dataset distributions to check coverage and extrapolation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05908db",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(231)\n",
    "plt.hist(X_train[predictor_features[0]], alpha = 0.2, color = 'red', edgecolor = 'black', bins=20)\n",
    "plt.title(predictor_features[0] + ' Train Dataset'); plt.xlim(fmin[pindex[0]],fmax[pindex[0]])\n",
    "\n",
    "plt.subplot(232)\n",
    "plt.hist(X_train[predictor_features[1]], alpha = 0.2, color = 'red', edgecolor = 'black', bins=20)\n",
    "plt.title(predictor_features[1] + ' Train Dataset'); plt.xlim(fmin[pindex[1]],fmax[pindex[1]])\n",
    "\n",
    "plt.subplot(233)\n",
    "plt.hist(y_train[response_feature], alpha = 0.2, color = 'red', edgecolor = 'black', bins=20)\n",
    "plt.title(response_feature + ' Train Dataset'); plt.xlim(fmin[rindex[0]],fmax[rindex[0]])\n",
    "\n",
    "plt.subplot(234)\n",
    "plt.hist(X_test[predictor_features[0]], alpha = 0.2, color = 'red', edgecolor = 'black', bins=20)\n",
    "plt.title(predictor_features[0] + ' Test Dataset'); plt.xlim(fmin[pindex[0]],fmax[pindex[0]])\n",
    "\n",
    "plt.subplot(235)\n",
    "plt.hist(X_test[predictor_features[1]], alpha = 0.2, color = 'red', edgecolor = 'black', bins=20)\n",
    "plt.title(predictor_features[1] + ' Test Dataset'); plt.xlim(fmin[pindex[1]],fmax[pindex[1]])\n",
    "\n",
    "plt.subplot(236)\n",
    "plt.hist(y_test[response_feature], alpha = 0.2, color = 'red', edgecolor = 'black', bins=20)\n",
    "plt.title(response_feature + ' Test Dataset'); plt.xlim(fmin[rindex[0]],fmax[rindex[0]])\n",
    "\n",
    "plt.subplots_adjust(left=0.0, bottom=0.0, right=2.2, top=1.2, wspace=0.3, hspace=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe4d816",
   "metadata": {},
   "source": [
    "The distributions are well behaved, we cannot observe obvious gaps nor truncations.  \n",
    "\n",
    "Let's look at a scatter plot of Predictor Feature 1 vs 2 with points colored by Response Feature.  \n",
    "\n",
    "* Let's plot the training and testing datasets to check coverage and extrapolation in the features space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e376f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(121)\n",
    "im = plt.scatter(X_train[predictor_features[0]],X_train[predictor_features[1]],s=None, c=y_train[response_feature], marker=None, cmap=None, norm=None, vmin=None, vmax=None, alpha=0.8, linewidths=0.3,  edgecolors=\"black\")\n",
    "plt.title('Training Data: ' + response_feature + ' vs. ' + predictor_features[1] + ' and ' + predictor_features[0]); plt.xlabel(predictor_features[0]); plt.ylabel(predictor_features[1])\n",
    "cbar = plt.colorbar(im, orientation = 'vertical',ticks=np.linspace(fmin[rindex[0]],fmax[rindex[0]], 10));\n",
    "plt.xlim(fmin[pindex[0]],fmax[pindex[0]]); plt.ylim(fmin[pindex[1]],fmax[pindex[1]])\n",
    "cbar.set_label(response_feature, rotation=270, labelpad=20)\n",
    "\n",
    "plt.subplot(122)\n",
    "im = plt.scatter(X_test[predictor_features[0]],X_test[predictor_features[1]],s=None, c=y_test[response_feature], marker=None, cmap=None, norm=None, vmin=None, vmax=None, alpha=0.8, linewidths=0.3, edgecolors=\"black\")\n",
    "plt.title('Testing Data: ' + response_feature + ' vs. ' + predictor_features[1] + ' and ' + predictor_features[0]); plt.xlabel(predictor_features[0]); plt.ylabel(predictor_features[1])\n",
    "cbar = plt.colorbar(im, orientation = 'vertical',ticks=np.linspace(fmin[rindex[0]],fmax[rindex[0]], 10)); \n",
    "plt.xlim(fmin[pindex[0]],fmax[pindex[0]]); plt.ylim(fmin[pindex[1]],fmax[pindex[1]])\n",
    "cbar.set_label(response_feature, rotation=270, labelpad=20)\n",
    "\n",
    "plt.subplots_adjust(left=0.0, bottom=0.0, right=2.2, top=1.2, wspace=0.3, hspace=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7df2be",
   "metadata": {},
   "source": [
    "#### Building a Linear Regression Model\n",
    "\n",
    "Let's build our first machine learning model with scikit learn.  We will start with linear regression.  For this model we will pick one predictor feature and one response feature.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71da85a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression Model with scikit learn\n",
    "from sklearn import linear_model\n",
    "\n",
    "# Step 1. Instantiate the Model \n",
    "univariate_linear_reg = linear_model.LinearRegression()\n",
    "\n",
    "# Step 2: Fit the Data on Training Data\n",
    "univariate_linear_reg.fit(X_train[predictor_features[0]].values.reshape(n_train,1), y_train[response_feature]) # fit model\n",
    "univariate_linear_model = np.linspace(fmin[pindex[0]],fmax[pindex[0]],10)\n",
    "\n",
    "# Print the model parameters\n",
    "response_model = univariate_linear_reg.predict(univariate_linear_model.reshape(10,1)) # predict with the fit model\n",
    "print('Coefficients: ', str(round(univariate_linear_reg.coef_[0],3)) + ', Intercept: ', str(round(univariate_linear_reg.intercept_,3))) \n",
    "\n",
    "# Plot model fit\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(X_train[predictor_features[0]].values, y_train[response_feature],  color='black', s = 20, alpha = 0.3)\n",
    "plt.plot(univariate_linear_model,response_model, color='red', linewidth=1)\n",
    "plt.title('Linear Regression Production from ' + predictor_features[0] + ' on Training'); plt.xlabel(predictor_features[0]); plt.ylabel('Production (MCFPD)')\n",
    "plt.xlim(fmin[pindex[0]],fmax[pindex[0]]); plt.ylim(fmin[rindex[0]],fmax[rindex[0]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1421e5da",
   "metadata": {},
   "source": [
    "We can now check our model performance against the withheld testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd5502c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: - Make predictions using the testing dataset\n",
    "y_pred = univariate_linear_reg.predict(X_test[predictor_features[0]].values.reshape(n_test,1))\n",
    "\n",
    "# Report the goodness of fit\n",
    "print('Variance explained: %.2f' % r2_score(y_test, y_pred))\n",
    "\n",
    "# Plot testing diagnostics \n",
    "plt.subplot(121)\n",
    "plt.scatter(X_test[predictor_features[0]].values, y_test[response_feature].values,  color='black', s = 20, alpha = 0.3)\n",
    "plt.scatter(X_test[predictor_features[0]], y_pred, color='blue', s = 20, alpha = 0.3)\n",
    "plt.title('Linear Regression Model Testing - ' + response_feature + ' from ' + predictor_features[0]); \n",
    "plt.xlabel(predictor_features[0]); plt.ylabel(response_feature)\n",
    "plt.xlim(fmin[pindex[0]],fmax[pindex[0]]); plt.ylim(fmin[rindex[0]],fmax[rindex[0]])\n",
    "\n",
    "y_res = y_pred - y_test[response_feature].values\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.hist(y_res, alpha = 0.2, color = 'red', edgecolor = 'black', bins=20)\n",
    "plt.title('Linear Regression Model Prediction Error - ' + response_feature + ' from ' + predictor_features[0]); \n",
    "plt.xlabel(response_feature + ' Estimation Error'); plt.ylabel('Frequency')\n",
    "plt.xlim(-1*fmax[rindex[0]],fmax[rindex[0]])\n",
    "\n",
    "plt.subplots_adjust(left=0.0, bottom=0.0, right=2.2, top=1.2, wspace=0.3, hspace=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ef645c",
   "metadata": {},
   "source": [
    "Now let's see how our linear regression model performs with just the second predictor feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4beca844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1. Instantiate the Model \n",
    "univariate_linear_reg2 = linear_model.LinearRegression()\n",
    "\n",
    "# Step 2: Fit the Data on Training Data\n",
    "univariate_linear_reg2.fit(X_train[predictor_features[1]].values.reshape(n_train,1), y_train[response_feature]) # fit model\n",
    "univariate_linear_model2 = np.linspace(fmin[pindex[1]],fmax[pindex[1]],10)\n",
    "\n",
    "# Print the model parameters\n",
    "response_model2 = univariate_linear_reg2.predict(univariate_linear_model2.reshape(10,1)) # predict with the fit model\n",
    "print('Coefficients: ', str(round(univariate_linear_reg.coef_[0],3)) + ', Intercept: ', str(round(univariate_linear_reg.intercept_,3))) \n",
    "\n",
    "# Plot model fit\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(X_train[predictor_features[1]].values, y_train[response_feature],  color='black', s = 20, alpha = 0.3)\n",
    "plt.plot(univariate_linear_model2,response_model2, color='red', linewidth=1)\n",
    "plt.title('Linear Regression Production from ' + predictor_features[1] + ' on Training'); plt.xlabel(predictor_features[1]); plt.ylabel('Production (MCFPD)')\n",
    "plt.xlim(fmin[pindex[1]],fmax[pindex[1]]); plt.ylim(fmin[rindex[0]],fmax[rindex[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc75f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: - Make predictions using the testing dataset\n",
    "y_pred2 = univariate_linear_reg2.predict(X_test[predictor_features[1]].values.reshape(n_test,1))\n",
    "\n",
    "# Report the goodness of fit\n",
    "print('Variance explained: %.2f' % r2_score(y_test, y_pred2))\n",
    "\n",
    "# Plot testing diagnostics \n",
    "plt.subplot(121)\n",
    "plt.scatter(X_test[predictor_features[1]].values, y_test[response_feature].values,  color='black', s = 20, alpha = 0.3)\n",
    "plt.scatter(X_test[predictor_features[1]], y_pred2, color='blue', s = 20, alpha = 0.3)\n",
    "plt.title('Linear Regression Model Testing - ' + response_feature + ' from ' + predictor_features[1]); \n",
    "plt.xlabel(predictor_features[1]); plt.ylabel(response_feature)\n",
    "plt.xlim(fmin[pindex[1]],fmax[pindex[1]]); plt.ylim(fmin[rindex[0]],fmax[rindex[0]])\n",
    "\n",
    "y_res2 = y_pred2 - y_test[response_feature].values\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.hist(y_res2, alpha = 0.2, color = 'red', edgecolor = 'black', bins=20)\n",
    "plt.title('Linear Regression Model Prediction Error - ' + response_feature + ' from ' + predictor_features[1]); \n",
    "plt.xlabel(response_feature + ' Estimation Error'); plt.ylabel('Frequency')\n",
    "plt.xlim(-1*fmax[rindex[0]],fmax[rindex[0]])\n",
    "\n",
    "plt.subplots_adjust(left=0.0, bottom=0.0, right=2.2, top=1.2, wspace=0.3, hspace=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40028cab",
   "metadata": {},
   "source": [
    "#### Building a Multilinear Regression Model\n",
    "\n",
    "Let's build our second machine learning model with scikit learn.  We will work with multilinear regression!  We will use both predictor features, porosity and brittleness, and one response feature, production.  \n",
    "\n",
    "There are no hyperparameters to tune with regular linear regression.\n",
    "\n",
    "* note: ridge regression and LASSO offer alternatives to linear regression with regularization coefficient, a hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea5dd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression Model with scikit learn\n",
    "from sklearn import linear_model\n",
    "\n",
    "# Step 1. Instantiate the Model \n",
    "multilinear_reg = linear_model.LinearRegression()\n",
    "\n",
    "# Step 2: Fit the Data on Training Data\n",
    "multilinear_reg.fit(X_train.values.reshape(n_train,2), y_train[response_feature]) # fit model\n",
    "\n",
    "# Print the model parameters\n",
    "print(predictor_features[0] + ' Coef: ' + str(round(multilinear_reg.coef_[0],3)) + ', ' + predictor_features[1] + ' Coef: ', str(round(multilinear_reg.coef_[1],3)) + ', Intercept: ', str(round(multilinear_reg.intercept_,3))) \n",
    "\n",
    "# Plot model fit\n",
    "plt.subplot(121)\n",
    "plt = visualize_model(multilinear_reg,X_test[predictor_features[0]],X_test[predictor_features[1]],y_test[response_feature],'Testing Data and Multilinear Regression Model')\n",
    "\n",
    "# Step 3: - Make predictions using the testing dataset\n",
    "multilinear_y_pred = multilinear_reg.predict(X_test.values.reshape(n_test,2))\n",
    "\n",
    "# Report the goodness of fit\n",
    "print('Variance explained: %.2f' % r2_score(y_test, multilinear_y_pred))\n",
    "\n",
    "# Calculate the error at withheld testing samples\n",
    "multilinear_y_res = multilinear_y_pred - y_test[response_feature].values\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.hist(multilinear_y_res, alpha = 0.2, color = 'red', edgecolor = 'black', bins=np.linspace(-1*fmax[rindex[0]],fmax[rindex[0]],40))\n",
    "plt.title('Linear Regression Model Prediction Error - ' + response_feature + ' from ' + predictor_features[0] + ' and ' + predictor_features[1]); \n",
    "plt.xlabel(response_feature + ' Estimation Error '); plt.ylabel('Frequency')\n",
    "plt.xlim(-1*fmax[rindex[0]],fmax[rindex[0]])\n",
    "\n",
    "plt.subplots_adjust(left=0.0, bottom=0.0, right=3.2, top=1.6, wspace=0.1, hspace=0.2)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b6bbc9",
   "metadata": {},
   "source": [
    "Including brittleness only resulted in a slight improvement.  \n",
    "\n",
    "* due to the nonlinear nature of brittleness\n",
    "\n",
    "#### Building a Decision Tree Regression Model\n",
    "\n",
    "Let's build our third machine learning model with scikit learn.  We will work with a decision tree.  We will use both predictor features, porosity and brittleness, and one response feature, production.\n",
    "\n",
    "The hyperparameters include:\n",
    "\n",
    "* **min_samples_leaf** - the minimum number of data in each region, reduce to increase complexity\n",
    "* **max_depth** - maximum number of layers of decisions, increase to increase complexity\n",
    "* **max_leaf_nodes** - maximum number of regions, increase to increase complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5656d45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decison Tree Model with scikit learn\n",
    "from sklearn import tree                                     # for accessing tree information\n",
    "\n",
    "# Step 1. Instantiate the Model \n",
    "decision_tree_reg = tree.DecisionTreeRegressor(min_samples_leaf=5, max_depth = 3)\n",
    "\n",
    "# Step 2: Fit the Data on Training Data\n",
    "decision_tree_reg.fit(X_train.values.reshape(n_train,2), y_train[response_feature]) # fit model\n",
    "\n",
    "# Plot model fit\n",
    "plt.subplot(121)\n",
    "plt = visualize_model(decision_tree_reg,X_test[predictor_features[0]],X_test[predictor_features[1]],y_test[response_feature],'Testing Data and Decision Tree Model')\n",
    "plt.subplots_adjust(left=0.0, bottom=0.0, right=1.2, top=1.2, wspace=0.3, hspace=0.2)\n",
    "\n",
    "# Step 3: - Make predictions using the testing dataset\n",
    "tree_y_pred = decision_tree_reg.predict(X_test.values.reshape(n_test,2))\n",
    "\n",
    "# Report the goodness of fit\n",
    "print('Variance explained: %.2f' % r2_score(y_test, tree_y_pred))\n",
    "\n",
    "# Calculate the error at withheld testing samples\n",
    "tree_y_res = tree_y_pred - y_test['Production'].values\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.hist(tree_y_res, alpha = 0.2, color = 'red', edgecolor = 'black', bins=np.linspace(-1*fmax[rindex[0]],fmax[rindex[0]],40))\n",
    "plt.title('Decision Tree Model Prediction Error - ' + response_feature + ' from ' + predictor_features[0] + ' and ' + predictor_features[1]); \n",
    "plt.xlabel(response_feature + ' Estimation Error '); plt.ylabel('Frequency')\n",
    "plt.xlim(-1*fmax[rindex[0]],fmax[rindex[0]])\n",
    "\n",
    "plt.subplots_adjust(left=0.0, bottom=0.0, right=3.2, top=1.6, wspace=0.1, hspace=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769e5e0b",
   "metadata": {},
   "source": [
    "#### Building a Support Vector Machine Regression Model\n",
    "\n",
    "Let's build our fourth machine learning model with scikit learn.  We will work with a support vector machine!  We will use both predictor features, porosity and brittleness, and one response feature, production.  \n",
    "\n",
    "The hyperparameters include:\n",
    "\n",
    "* **kernel** - try linear, poly and rbf (radial basis function), for poly increase degree to increase complexity\n",
    "* **C** - cost (inverse of margin width) - increase to increase complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f223a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support Vector Regression Model with scikit learn\n",
    "from sklearn import linear_model\n",
    "\n",
    "# Step 1. Instantiate the Model \n",
    "support_vector_reg = svm.SVR(kernel='poly', C=0.010, gamma='auto', degree=2, epsilon=.01,coef0=1,max_iter=1000)\n",
    "\n",
    "# Step 2: Fit the Data on Training Data\n",
    "support_vector_reg.fit(X_train.values.reshape(n_train,2), y_train[response_feature]) # fit model\n",
    "\n",
    "# Plot model fit\n",
    "plt.subplot(121)\n",
    "plt = visualize_model(support_vector_reg,X_test[predictor_features[0]],X_test[predictor_features[1]],y_test[response_feature],'Testing Data and Support Vector Model')\n",
    "\n",
    "# Step 3: - Make predictions using the testing dataset\n",
    "svm_y_pred = support_vector_reg.predict(X_test.values.reshape(n_test,2))\n",
    "\n",
    "# Report the goodness of fit\n",
    "print('Variance explained: %.2f' % r2_score(y_test, svm_y_pred))\n",
    "\n",
    "# Calculate the error at withheld testing samples\n",
    "svm_y_res = svm_y_pred - y_test['Production'].values\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.hist(svm_y_res, alpha = 0.2, color = 'red', edgecolor = 'black', bins=np.linspace(-1*fmax[rindex[0]],fmax[rindex[0]],40))\n",
    "plt.title('Support Vector Machine Model Prediction Error - ' + response_feature + ' from ' + predictor_features[0] + ' and ' + predictor_features[1]); \n",
    "plt.xlabel(response_feature + ' Estimation Error '); plt.ylabel('Frequency')\n",
    "plt.xlim(-1*fmax[rindex[0]],fmax[rindex[0]])\n",
    "\n",
    "plt.subplots_adjust(left=0.0, bottom=0.0, right=3.2, top=1.6, wspace=0.1, hspace=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120b451f",
   "metadata": {},
   "source": [
    "Let's put all our models together for one visualization of our:\n",
    "\n",
    "* models with the withheld testing data\n",
    "\n",
    "* error at withheld testing data samples distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afb2b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(231)\n",
    "plt = visualize_model(multilinear_reg,X_test[predictor_features[0]],X_test[predictor_features[1]],y_test[response_feature],'Training Data and Multilinear Regression Model')\n",
    "\n",
    "plt.subplot(234)\n",
    "plt.hist(multilinear_y_res, alpha = 0.2, color = 'red', edgecolor = 'black', bins=np.linspace(-1*fmax[rindex[0]],fmax[rindex[0]],40))\n",
    "plt.title('Linear Regression Model Prediction Error - ' + response_feature + ' from ' + predictor_features[0] + ' and ' + predictor_features[1]); \n",
    "plt.xlabel(response_feature + ' Estimation Error '); plt.ylabel('Frequency')\n",
    "plt.xlim(-1*fmax[rindex[0]],fmax[rindex[0]])\n",
    "\n",
    "plt.subplot(232)\n",
    "plt = visualize_model(decision_tree_reg,X_test[predictor_features[0]],X_test[predictor_features[1]],y_test[response_feature],'Training Data and Decision Tree Model')\n",
    "\n",
    "plt.subplot(235)\n",
    "plt.hist(tree_y_res, alpha = 0.2, color = 'red', edgecolor = 'black', bins=np.linspace(-1*fmax[rindex[0]],fmax[rindex[0]],40))\n",
    "plt.title('Decision Tree Model Prediction Error - ' + response_feature + ' from ' + predictor_features[0] + ' and ' + predictor_features[1]); \n",
    "plt.xlabel(response_feature + ' Estimation Error '); plt.ylabel('Frequency')\n",
    "plt.xlim(-1*fmax[rindex[0]],fmax[rindex[0]])\n",
    "\n",
    "plt.subplot(233)\n",
    "plt = visualize_model(support_vector_reg,X_test[predictor_features[0]],X_test[predictor_features[1]],y_test[response_feature],'Training Data and Support Vector Model')\n",
    "\n",
    "plt.subplot(236)\n",
    "plt.hist(svm_y_res, alpha = 0.2, color = 'red', edgecolor = 'black', bins=np.linspace(-1*fmax[rindex[0]],fmax[rindex[0]],40))\n",
    "plt.title('Support Vector Machine Model Prediction Error - ' + response_feature + ' from ' + predictor_features[0] + ' and ' + predictor_features[1]); \n",
    "plt.xlabel(response_feature + ' Estimation Error '); plt.ylabel('Frequency')\n",
    "plt.xlim(-1*fmax[rindex[0]],fmax[rindex[0]])\n",
    "\n",
    "plt.subplots_adjust(left=0.0, bottom=0.0, right=3.2, top=2.6, wspace=0.1, hspace=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24034d97",
   "metadata": {},
   "source": [
    "I'll end here for brevity, but I invite you to continue. There are many other scikit learn methods to explore and there are further opportunities for model cross validation and hyperparameter tuning.  \n",
    "\n",
    "* return to the beginning and select a different pair of predictor features and try again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411e2323",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb9e480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 'your_data.csv' with the path to your dataset\n",
    "file_path = 'Cleaned_HackathonData2024.csv'\n",
    "data = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f677ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating numeric and non-numeric columns\n",
    "numeric_data = data.select_dtypes(include=['float64', 'int64'])\n",
    "non_numeric_data = data.select_dtypes(exclude=['float64', 'int64'])\n",
    "\n",
    "# Imputers for different types of data\n",
    "numeric_imputer = SimpleImputer(strategy='mean')\n",
    "non_numeric_imputer = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "# Imputation\n",
    "numeric_data_imputed = pd.DataFrame(numeric_imputer.fit_transform(numeric_data), columns=numeric_data.columns)\n",
    "non_numeric_data_imputed = pd.DataFrame(non_numeric_imputer.fit_transform(non_numeric_data), columns=non_numeric_data.columns)\n",
    "\n",
    "# Combine the data back\n",
    "data_imputed_combined = pd.concat([numeric_data_imputed, non_numeric_data_imputed], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b32baa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistical analysis on the imputed numeric data\n",
    "statistical_analysis_numeric = numeric_data_imputed.describe()\n",
    "statistical_analysis_numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf5d386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of the completeness of the dataset before and after imputation\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Before imputation\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.heatmap(data.isna(), cbar=False)\n",
    "plt.title('Before Imputation')\n",
    "plt.xlabel('Columns')\n",
    "plt.ylabel('Index')\n",
    "\n",
    "# After imputation\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.heatmap(data_imputed_combined.isna(), cbar=False)\n",
    "plt.title('After Imputation')\n",
    "plt.xlabel('Columns')\n",
    "plt.ylabel('Index')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2553d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scikit-Learn Demonstration\n",
    "\n",
    "Demonstration of scikit learn for machine learning.\n",
    "\n",
    "In this workflow we demonstrate the plug and play nature of scikit learn machine learning models.  \n",
    "\n",
    "For an unconventional dataset we demonstrate the following steps:\n",
    "    \n",
    "    1. instantiation\n",
    "    2. fitting\n",
    "    3. prediction\n",
    "    4. cross validation\n",
    "    \n",
    "We will work with the following regression methods:\n",
    "\n",
    "* linear regression\n",
    "* multilinear regression\n",
    "* decision tree regression\n",
    "* support vector machine regression\n",
    "\n",
    "We will demonstrate together a model and then you will pick any 2 predictor features and build your own model.\n",
    "\n",
    "* try some new hyperparameters and observe the model performance\n",
    "\n",
    "We will also need some standard packages.\n",
    "\n",
    "import os                                                 # to set current working directory \n",
    "import math                                               # basic calculations like square root\n",
    "from sklearn.model_selection import train_test_split      # train and test split\n",
    "from sklearn import svm                                   # support vector machine methods\n",
    "from sklearn import tree                                  # tree program from scikit learn (package for machine learning)\n",
    "from sklearn.metrics import mean_squared_error, r2_score  # specific measures to check our models\n",
    "from sklearn.impute import SimpleImputer\n",
    "import pandas as pd                                       # DataFrames and plotting\n",
    "import pandas.plotting as pd_plot\n",
    "import numpy as np                                        # arrays and matrix math\n",
    "import matplotlib.pyplot as plt                           # plotting\n",
    "from intake import cat                                    # data catalogue\n",
    "\n",
    "#### Declare functions\n",
    "\n",
    "Let's define a couple of functions to streamline plotting correlation matrices and visualization of a machine learning regression model responce over the 2 predictor features. \n",
    "\n",
    "def plot_corr(dataframe,size=10):                         # plots a graphical correlation matrix \n",
    "    corr = dataframe.corr()\n",
    "    fig, ax = plt.subplots(figsize=(size, size))\n",
    "    im = ax.matshow(corr,vmin = -1.0, vmax = 1.0)\n",
    "    plt.xticks(range(len(corr.columns)), corr.columns);\n",
    "    plt.yticks(range(len(corr.columns)), corr.columns);\n",
    "    plt.colorbar(im, orientation = 'vertical')\n",
    "    plt.title('Correlation Matrix')\n",
    "    \n",
    "def visualize_model(model,xfeature,yfeature,response,title,):# plots the data points and the decision tree prediction \n",
    "    n_classes = 10\n",
    "    cmap = plt.cm.RdYlBu\n",
    "    plot_step = 0.02\n",
    "    x_min, x_max = min(xfeature) - 1, max(xfeature) + 1\n",
    "    y_min, y_max = min(yfeature) - 1, max(yfeature) + 1\n",
    "    resp_min = round(min(response)); resp_max = round(max(response));\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),\n",
    "                     np.arange(y_min, y_max, plot_step))\n",
    "    z_min = round(min(response)); z_max = round(max(response))\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    cs = plt.contourf(xx, yy, Z, cmap=cmap,vmin=z_min, vmax=z_max)\n",
    "\n",
    "    im = plt.scatter(xfeature,yfeature,s=None, c=response, marker=None, cmap=cmap, norm=None, vmin=z_min, vmax=z_max, alpha=0.8, linewidths=0.3, edgecolors=\"black\")\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xfeature.name)\n",
    "    plt.ylabel(yfeature.name)\n",
    "    cbar = plt.colorbar(im, orientation = 'vertical')\n",
    "    cbar.set_label(response.name, rotation=270, labelpad=20)\n",
    "    return(plt)\n",
    "\n",
    "\n",
    "#### Read the data table\n",
    "\n",
    "my_data = pd.read_csv(\"Numerical_HackathonData2024.csv\")\n",
    "\n",
    "Let's visualize the first several rows of our data stored in a DataFrame so we can make sure we successfully loaded the data file.\n",
    "\n",
    "my_data.head()                                            # preview the first 5 rows of the dataframe\n",
    "\n",
    "Let's remove the well index and check the summary summary statistics.\n",
    "\n",
    "my_data = my_data.iloc[:,1:33]                             # copy all rows and columns 1 through 8, note 0 column is removed\n",
    "my_data.describe().transpose()                            # calculate summary statistics for the data\n",
    "\n",
    "It is good that we checked the summary statistics, because we have some negative values for brittleness and total organic carbon. The is physically imposible.  The values must be in error. We know the lowest possible values are 0.0, so we will truncate on 0.0.  We use the *get_numerical_data()* DataFrame member function to get a shallow copy of the data from the DataFrame.  Since it is a shallow copy, any changes we make to the copy are made to the data in the original DataFrame.  This allows us to apply this simple conditional statement to all the data values in the DataFrame all at once.\n",
    "\n",
    "#num = my_data._get_numeric_data()                         # get the numerical values\n",
    "#num[num < 0] = 0                                          # truncate negative values to 0.0\n",
    "#my_data.describe().transpose()                            # calculate summary statistics for the data\n",
    "\n",
    "Let's assign min and max values for visualization\n",
    "\n",
    "fmin = [-15,2200,7250,0,250,125,140]\n",
    "fmax = [125,18500,11000,10,3000,3000,3000]\n",
    "\n",
    "This dataset has variables from 1,000 unconventional wells including well average porosity, log transform of permeability (to linearize the relationships with other variables), accoustic impedance (kg/m2s*10^6), brittness ratio (%), total organic carbon (%), vitrinite reflectance (%), and initial production 90 day average (MCFPD).  Note, the dataset is synthetic.\n",
    "\n",
    "#### Calculate the correlation matrix \n",
    "\n",
    "For multivariate analysis it is a good idea to check the correlation matrix.  We can calculate it and view it in the console with these commands.\n",
    "\n",
    "imputer = SimpleImputer(strategy='mean')  # or another appropriate strategy\n",
    "my_data_imputed = pd.DataFrame(imputer.fit_transform(my_data), columns=my_data.columns)\n",
    "\n",
    "corr_matrix = np.corrcoef(my_data_imputed, rowvar = False)\n",
    "print(np.around(corr_matrix,2))                           # print the correlation matrix to 2 decimals\n",
    "\n",
    "Note the 1.0 diagonal resulting from the correlation of each variable with themselves.  \n",
    "\n",
    "Let's use our function declared above to make a graphical correlation matrix visualization.  This may inprove our ability to spot features.  It relies on the built in correlation matrix method with Numpy DataFrames and MatPlotLib for plotting.\n",
    "\n",
    "plot_corr(my_data,10)                                      # using our correlation matrix visualization function\n",
    "plt.show()\n",
    "\n",
    "This looks good.  There is a mix of correlation magnitudes. Of course, correlation coeffficients are limited to degree of linear correlations.  For more complete information, let's look at the matrix scatter plot from the Pandas package. \n",
    "\n",
    "pd_plot.scatter_matrix(my_data, alpha = 0.1,              # pandas matrix scatter plot\n",
    "    figsize=(10, 10),color = 'black', hist_kwds={'color':['grey']})\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "#### Working with Only Two Predictor Features to Predict One Response Feature\n",
    "\n",
    "Let's simplify the problem to 2 features, to predict one response feature, Production rate.  We will also reduce the number of wells from 1,000 to 500.  By working with only 2 predictor features, it is very easy to visualize the segmentation of the feature space (it is only 2D and the model can be interogated exhaustively on a single plot).\n",
    "\n",
    "We will demonstrate with:\n",
    "\n",
    "* Porosity and Brittleness\n",
    "\n",
    "to predict:\n",
    "\n",
    "* Production\n",
    "\n",
    "Then you will have a chance to run the workflow again with your own selection for predictor features.\n",
    "\n",
    "To update the predictor features change the code in the box below:\n",
    "\n",
    "```python\n",
    "predictor_features = ['Por','Brittle'] \n",
    "```\n",
    "\n",
    "You can any of these:\n",
    "\n",
    "1. Por\n",
    "2. LogPerm\n",
    "3. AI\n",
    "4. Brittle\n",
    "5. TOC\n",
    "6. VR\n",
    "\n",
    "Note the feature names are caps sensitive.\n",
    "\n",
    "# Select 2 predictor features here:\n",
    "predictor_features = ['Por','Brittle']           # for the first demonstration run we will use porosity and brittleness\n",
    "#\n",
    "\n",
    "response_feature = 'Production'\n",
    "\n",
    "pindex = np.argwhere(my_data.columns.isin(predictor_features)).ravel()\n",
    "print('Selected predictor features: \\n0: ' + predictor_features[0] + ', index = ' + str(pindex[0]) + '.')\n",
    "print('1: ' + predictor_features[1] + ', index = ' + str(pindex[1]) + '.')\n",
    "rindex = np.argwhere(my_data.columns.isin([response_feature])).ravel()\n",
    "print('\\nSelected response feature: \\n' + response_feature + ', index = ' + str(rindex[0]) + '.')\n",
    "\n",
    "#my_data_cut = my_data.iloc[0:500,:] \n",
    "X = my_data[predictor_features]                # extract the 2 selected response features, 500 samples to a X array\n",
    "y = my_data[[response_feature]]                # extract selected response 500 samples to a response array\n",
    "\n",
    "Let's check the summary statistics of Predictor Features 1 and 2 and the Response Feature.\n",
    "\n",
    "X.describe().transpose()                   # calculate summary statistics for the data\n",
    "\n",
    "y.describe().transpose()                   # calculate summary statistics for the data\n",
    "\n",
    "Now let's withhold 100 samples as testing data and retain the remaining 400 as training data. \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=73073)\n",
    "n_train = len(X_train)\n",
    "n_test = len(X_test)\n",
    "print('Number of training ' + str(n_train) + ', number of test ' + str(n_test))\n",
    "\n",
    "Let's compare the univariate statistics of Predictor Features 1 and 2 and the Response Feature for the training and testing datasets.  \n",
    " \n",
    " * let's check for bias and extrapolation.\n",
    "\n",
    "X_train.describe().transpose()                   # calculate summary statistics for the data\n",
    "\n",
    "X_test.describe().transpose()                   # calculate summary statistics for the data\n",
    "\n",
    "y_train.describe().transpose()                   # calculate summary statistics for the data\n",
    "\n",
    "y_test.describe().transpose()                   # calculate summary statistics for the data\n",
    "\n",
    "Now let's plot the training and testing dataset distributions to check coverage and extrapolation.\n",
    "\n",
    "plt.subplot(231)\n",
    "plt.hist(X_train[predictor_features[0]], alpha = 0.2, color = 'red', edgecolor = 'black', bins=20)\n",
    "plt.title(predictor_features[0] + ' Train Dataset'); plt.xlim(fmin[pindex[0]],fmax[pindex[0]])\n",
    "\n",
    "plt.subplot(232)\n",
    "plt.hist(X_train[predictor_features[1]], alpha = 0.2, color = 'red', edgecolor = 'black', bins=20)\n",
    "plt.title(predictor_features[1] + ' Train Dataset'); plt.xlim(fmin[pindex[1]],fmax[pindex[1]])\n",
    "\n",
    "plt.subplot(233)\n",
    "plt.hist(y_train[response_feature], alpha = 0.2, color = 'red', edgecolor = 'black', bins=20)\n",
    "plt.title(response_feature + ' Train Dataset'); plt.xlim(fmin[rindex[0]],fmax[rindex[0]])\n",
    "\n",
    "plt.subplot(234)\n",
    "plt.hist(X_test[predictor_features[0]], alpha = 0.2, color = 'red', edgecolor = 'black', bins=20)\n",
    "plt.title(predictor_features[0] + ' Test Dataset'); plt.xlim(fmin[pindex[0]],fmax[pindex[0]])\n",
    "\n",
    "plt.subplot(235)\n",
    "plt.hist(X_test[predictor_features[1]], alpha = 0.2, color = 'red', edgecolor = 'black', bins=20)\n",
    "plt.title(predictor_features[1] + ' Test Dataset'); plt.xlim(fmin[pindex[1]],fmax[pindex[1]])\n",
    "\n",
    "plt.subplot(236)\n",
    "plt.hist(y_test[response_feature], alpha = 0.2, color = 'red', edgecolor = 'black', bins=20)\n",
    "plt.title(response_feature + ' Test Dataset'); plt.xlim(fmin[rindex[0]],fmax[rindex[0]])\n",
    "\n",
    "plt.subplots_adjust(left=0.0, bottom=0.0, right=2.2, top=1.2, wspace=0.3, hspace=0.3)\n",
    "plt.show()\n",
    "\n",
    "The distributions are well behaved, we cannot observe obvious gaps nor truncations.  \n",
    "\n",
    "Let's look at a scatter plot of Predictor Feature 1 vs 2 with points colored by Response Feature.  \n",
    "\n",
    "* Let's plot the training and testing datasets to check coverage and extrapolation in the features space.\n",
    "\n",
    "plt.subplot(121)\n",
    "im = plt.scatter(X_train[predictor_features[0]],X_train[predictor_features[1]],s=None, c=y_train[response_feature], marker=None, cmap=None, norm=None, vmin=None, vmax=None, alpha=0.8, linewidths=0.3,  edgecolors=\"black\")\n",
    "plt.title('Training Data: ' + response_feature + ' vs. ' + predictor_features[1] + ' and ' + predictor_features[0]); plt.xlabel(predictor_features[0]); plt.ylabel(predictor_features[1])\n",
    "cbar = plt.colorbar(im, orientation = 'vertical',ticks=np.linspace(fmin[rindex[0]],fmax[rindex[0]], 10));\n",
    "plt.xlim(fmin[pindex[0]],fmax[pindex[0]]); plt.ylim(fmin[pindex[1]],fmax[pindex[1]])\n",
    "cbar.set_label(response_feature, rotation=270, labelpad=20)\n",
    "\n",
    "plt.subplot(122)\n",
    "im = plt.scatter(X_test[predictor_features[0]],X_test[predictor_features[1]],s=None, c=y_test[response_feature], marker=None, cmap=None, norm=None, vmin=None, vmax=None, alpha=0.8, linewidths=0.3, edgecolors=\"black\")\n",
    "plt.title('Testing Data: ' + response_feature + ' vs. ' + predictor_features[1] + ' and ' + predictor_features[0]); plt.xlabel(predictor_features[0]); plt.ylabel(predictor_features[1])\n",
    "cbar = plt.colorbar(im, orientation = 'vertical',ticks=np.linspace(fmin[rindex[0]],fmax[rindex[0]], 10)); \n",
    "plt.xlim(fmin[pindex[0]],fmax[pindex[0]]); plt.ylim(fmin[pindex[1]],fmax[pindex[1]])\n",
    "cbar.set_label(response_feature, rotation=270, labelpad=20)\n",
    "\n",
    "plt.subplots_adjust(left=0.0, bottom=0.0, right=2.2, top=1.2, wspace=0.3, hspace=0.2)\n",
    "plt.show()\n",
    "\n",
    "#### Building a Linear Regression Model\n",
    "\n",
    "Let's build our first machine learning model with scikit learn.  We will start with linear regression.  For this model we will pick one predictor feature and one response feature.  \n",
    "\n",
    "# Linear Regression Model with scikit learn\n",
    "from sklearn import linear_model\n",
    "\n",
    "# Step 1. Instantiate the Model \n",
    "univariate_linear_reg = linear_model.LinearRegression()\n",
    "\n",
    "# Step 2: Fit the Data on Training Data\n",
    "univariate_linear_reg.fit(X_train[predictor_features[0]].values.reshape(n_train,1), y_train[response_feature]) # fit model\n",
    "univariate_linear_model = np.linspace(fmin[pindex[0]],fmax[pindex[0]],10)\n",
    "\n",
    "# Print the model parameters\n",
    "response_model = univariate_linear_reg.predict(univariate_linear_model.reshape(10,1)) # predict with the fit model\n",
    "print('Coefficients: ', str(round(univariate_linear_reg.coef_[0],3)) + ', Intercept: ', str(round(univariate_linear_reg.intercept_,3))) \n",
    "\n",
    "# Plot model fit\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(X_train[predictor_features[0]].values, y_train[response_feature],  color='black', s = 20, alpha = 0.3)\n",
    "plt.plot(univariate_linear_model,response_model, color='red', linewidth=1)\n",
    "plt.title('Linear Regression Production from ' + predictor_features[0] + ' on Training'); plt.xlabel(predictor_features[0]); plt.ylabel('Production (MCFPD)')\n",
    "plt.xlim(fmin[pindex[0]],fmax[pindex[0]]); plt.ylim(fmin[rindex[0]],fmax[rindex[0]])\n",
    "\n",
    "\n",
    "We can now check our model performance against the withheld testing data.\n",
    "\n",
    "# Step 3: - Make predictions using the testing dataset\n",
    "y_pred = univariate_linear_reg.predict(X_test[predictor_features[0]].values.reshape(n_test,1))\n",
    "\n",
    "# Report the goodness of fit\n",
    "print('Variance explained: %.2f' % r2_score(y_test, y_pred))\n",
    "\n",
    "# Plot testing diagnostics \n",
    "plt.subplot(121)\n",
    "plt.scatter(X_test[predictor_features[0]].values, y_test[response_feature].values,  color='black', s = 20, alpha = 0.3)\n",
    "plt.scatter(X_test[predictor_features[0]], y_pred, color='blue', s = 20, alpha = 0.3)\n",
    "plt.title('Linear Regression Model Testing - ' + response_feature + ' from ' + predictor_features[0]); \n",
    "plt.xlabel(predictor_features[0]); plt.ylabel(response_feature)\n",
    "plt.xlim(fmin[pindex[0]],fmax[pindex[0]]); plt.ylim(fmin[rindex[0]],fmax[rindex[0]])\n",
    "\n",
    "y_res = y_pred - y_test[response_feature].values\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.hist(y_res, alpha = 0.2, color = 'red', edgecolor = 'black', bins=20)\n",
    "plt.title('Linear Regression Model Prediction Error - ' + response_feature + ' from ' + predictor_features[0]); \n",
    "plt.xlabel(response_feature + ' Estimation Error'); plt.ylabel('Frequency')\n",
    "plt.xlim(-1*fmax[rindex[0]],fmax[rindex[0]])\n",
    "\n",
    "plt.subplots_adjust(left=0.0, bottom=0.0, right=2.2, top=1.2, wspace=0.3, hspace=0.2)\n",
    "plt.show()\n",
    "\n",
    "Now let's see how our linear regression model performs with just the second predictor feature.\n",
    "\n",
    "# Step 1. Instantiate the Model \n",
    "univariate_linear_reg2 = linear_model.LinearRegression()\n",
    "\n",
    "# Step 2: Fit the Data on Training Data\n",
    "univariate_linear_reg2.fit(X_train[predictor_features[1]].values.reshape(n_train,1), y_train[response_feature]) # fit model\n",
    "univariate_linear_model2 = np.linspace(fmin[pindex[1]],fmax[pindex[1]],10)\n",
    "\n",
    "# Print the model parameters\n",
    "response_model2 = univariate_linear_reg2.predict(univariate_linear_model2.reshape(10,1)) # predict with the fit model\n",
    "print('Coefficients: ', str(round(univariate_linear_reg.coef_[0],3)) + ', Intercept: ', str(round(univariate_linear_reg.intercept_,3))) \n",
    "\n",
    "# Plot model fit\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(X_train[predictor_features[1]].values, y_train[response_feature],  color='black', s = 20, alpha = 0.3)\n",
    "plt.plot(univariate_linear_model2,response_model2, color='red', linewidth=1)\n",
    "plt.title('Linear Regression Production from ' + predictor_features[1] + ' on Training'); plt.xlabel(predictor_features[1]); plt.ylabel('Production (MCFPD)')\n",
    "plt.xlim(fmin[pindex[1]],fmax[pindex[1]]); plt.ylim(fmin[rindex[0]],fmax[rindex[0]])\n",
    "\n",
    "# Step 3: - Make predictions using the testing dataset\n",
    "y_pred2 = univariate_linear_reg2.predict(X_test[predictor_features[1]].values.reshape(n_test,1))\n",
    "\n",
    "# Report the goodness of fit\n",
    "print('Variance explained: %.2f' % r2_score(y_test, y_pred2))\n",
    "\n",
    "# Plot testing diagnostics \n",
    "plt.subplot(121)\n",
    "plt.scatter(X_test[predictor_features[1]].values, y_test[response_feature].values,  color='black', s = 20, alpha = 0.3)\n",
    "plt.scatter(X_test[predictor_features[1]], y_pred2, color='blue', s = 20, alpha = 0.3)\n",
    "plt.title('Linear Regression Model Testing - ' + response_feature + ' from ' + predictor_features[1]); \n",
    "plt.xlabel(predictor_features[1]); plt.ylabel(response_feature)\n",
    "plt.xlim(fmin[pindex[1]],fmax[pindex[1]]); plt.ylim(fmin[rindex[0]],fmax[rindex[0]])\n",
    "\n",
    "y_res2 = y_pred2 - y_test[response_feature].values\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.hist(y_res2, alpha = 0.2, color = 'red', edgecolor = 'black', bins=20)\n",
    "plt.title('Linear Regression Model Prediction Error - ' + response_feature + ' from ' + predictor_features[1]); \n",
    "plt.xlabel(response_feature + ' Estimation Error'); plt.ylabel('Frequency')\n",
    "plt.xlim(-1*fmax[rindex[0]],fmax[rindex[0]])\n",
    "\n",
    "plt.subplots_adjust(left=0.0, bottom=0.0, right=2.2, top=1.2, wspace=0.3, hspace=0.2)\n",
    "plt.show()\n",
    "\n",
    "#### Building a Multilinear Regression Model\n",
    "\n",
    "Let's build our second machine learning model with scikit learn.  We will work with multilinear regression!  We will use both predictor features, porosity and brittleness, and one response feature, production.  \n",
    "\n",
    "There are no hyperparameters to tune with regular linear regression.\n",
    "\n",
    "* note: ridge regression and LASSO offer alternatives to linear regression with regularization coefficient, a hyperparameter.\n",
    "\n",
    "# Linear Regression Model with scikit learn\n",
    "from sklearn import linear_model\n",
    "\n",
    "# Step 1. Instantiate the Model \n",
    "multilinear_reg = linear_model.LinearRegression()\n",
    "\n",
    "# Step 2: Fit the Data on Training Data\n",
    "multilinear_reg.fit(X_train.values.reshape(n_train,2), y_train[response_feature]) # fit model\n",
    "\n",
    "# Print the model parameters\n",
    "print(predictor_features[0] + ' Coef: ' + str(round(multilinear_reg.coef_[0],3)) + ', ' + predictor_features[1] + ' Coef: ', str(round(multilinear_reg.coef_[1],3)) + ', Intercept: ', str(round(multilinear_reg.intercept_,3))) \n",
    "\n",
    "# Plot model fit\n",
    "plt.subplot(121)\n",
    "plt = visualize_model(multilinear_reg,X_test[predictor_features[0]],X_test[predictor_features[1]],y_test[response_feature],'Testing Data and Multilinear Regression Model')\n",
    "\n",
    "# Step 3: - Make predictions using the testing dataset\n",
    "multilinear_y_pred = multilinear_reg.predict(X_test.values.reshape(n_test,2))\n",
    "\n",
    "# Report the goodness of fit\n",
    "print('Variance explained: %.2f' % r2_score(y_test, multilinear_y_pred))\n",
    "\n",
    "# Calculate the error at withheld testing samples\n",
    "multilinear_y_res = multilinear_y_pred - y_test[response_feature].values\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.hist(multilinear_y_res, alpha = 0.2, color = 'red', edgecolor = 'black', bins=np.linspace(-1*fmax[rindex[0]],fmax[rindex[0]],40))\n",
    "plt.title('Linear Regression Model Prediction Error - ' + response_feature + ' from ' + predictor_features[0] + ' and ' + predictor_features[1]); \n",
    "plt.xlabel(response_feature + ' Estimation Error '); plt.ylabel('Frequency')\n",
    "plt.xlim(-1*fmax[rindex[0]],fmax[rindex[0]])\n",
    "\n",
    "plt.subplots_adjust(left=0.0, bottom=0.0, right=3.2, top=1.6, wspace=0.1, hspace=0.2)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "Including brittleness only resulted in a slight improvement.  \n",
    "\n",
    "* due to the nonlinear nature of brittleness\n",
    "\n",
    "#### Building a Decision Tree Regression Model\n",
    "\n",
    "Let's build our third machine learning model with scikit learn.  We will work with a decision tree.  We will use both predictor features, porosity and brittleness, and one response feature, production.\n",
    "\n",
    "The hyperparameters include:\n",
    "\n",
    "* **min_samples_leaf** - the minimum number of data in each region, reduce to increase complexity\n",
    "* **max_depth** - maximum number of layers of decisions, increase to increase complexity\n",
    "* **max_leaf_nodes** - maximum number of regions, increase to increase complexity\n",
    "\n",
    "# Decison Tree Model with scikit learn\n",
    "from sklearn import tree                                     # for accessing tree information\n",
    "\n",
    "# Step 1. Instantiate the Model \n",
    "decision_tree_reg = tree.DecisionTreeRegressor(min_samples_leaf=5, max_depth = 3)\n",
    "\n",
    "# Step 2: Fit the Data on Training Data\n",
    "decision_tree_reg.fit(X_train.values.reshape(n_train,2), y_train[response_feature]) # fit model\n",
    "\n",
    "# Plot model fit\n",
    "plt.subplot(121)\n",
    "plt = visualize_model(decision_tree_reg,X_test[predictor_features[0]],X_test[predictor_features[1]],y_test[response_feature],'Testing Data and Decision Tree Model')\n",
    "plt.subplots_adjust(left=0.0, bottom=0.0, right=1.2, top=1.2, wspace=0.3, hspace=0.2)\n",
    "\n",
    "# Step 3: - Make predictions using the testing dataset\n",
    "tree_y_pred = decision_tree_reg.predict(X_test.values.reshape(n_test,2))\n",
    "\n",
    "# Report the goodness of fit\n",
    "print('Variance explained: %.2f' % r2_score(y_test, tree_y_pred))\n",
    "\n",
    "# Calculate the error at withheld testing samples\n",
    "tree_y_res = tree_y_pred - y_test['Production'].values\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.hist(tree_y_res, alpha = 0.2, color = 'red', edgecolor = 'black', bins=np.linspace(-1*fmax[rindex[0]],fmax[rindex[0]],40))\n",
    "plt.title('Decision Tree Model Prediction Error - ' + response_feature + ' from ' + predictor_features[0] + ' and ' + predictor_features[1]); \n",
    "plt.xlabel(response_feature + ' Estimation Error '); plt.ylabel('Frequency')\n",
    "plt.xlim(-1*fmax[rindex[0]],fmax[rindex[0]])\n",
    "\n",
    "plt.subplots_adjust(left=0.0, bottom=0.0, right=3.2, top=1.6, wspace=0.1, hspace=0.2)\n",
    "plt.show()\n",
    "\n",
    "#### Building a Support Vector Machine Regression Model\n",
    "\n",
    "Let's build our fourth machine learning model with scikit learn.  We will work with a support vector machine!  We will use both predictor features, porosity and brittleness, and one response feature, production.  \n",
    "\n",
    "The hyperparameters include:\n",
    "\n",
    "* **kernel** - try linear, poly and rbf (radial basis function), for poly increase degree to increase complexity\n",
    "* **C** - cost (inverse of margin width) - increase to increase complexity\n",
    "\n",
    "# Support Vector Regression Model with scikit learn\n",
    "from sklearn import linear_model\n",
    "\n",
    "# Step 1. Instantiate the Model \n",
    "support_vector_reg = svm.SVR(kernel='poly', C=0.010, gamma='auto', degree=2, epsilon=.01,coef0=1,max_iter=1000)\n",
    "\n",
    "# Step 2: Fit the Data on Training Data\n",
    "support_vector_reg.fit(X_train.values.reshape(n_train,2), y_train[response_feature]) # fit model\n",
    "\n",
    "# Plot model fit\n",
    "plt.subplot(121)\n",
    "plt = visualize_model(support_vector_reg,X_test[predictor_features[0]],X_test[predictor_features[1]],y_test[response_feature],'Testing Data and Support Vector Model')\n",
    "\n",
    "# Step 3: - Make predictions using the testing dataset\n",
    "svm_y_pred = support_vector_reg.predict(X_test.values.reshape(n_test,2))\n",
    "\n",
    "# Report the goodness of fit\n",
    "print('Variance explained: %.2f' % r2_score(y_test, svm_y_pred))\n",
    "\n",
    "# Calculate the error at withheld testing samples\n",
    "svm_y_res = svm_y_pred - y_test['Production'].values\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.hist(svm_y_res, alpha = 0.2, color = 'red', edgecolor = 'black', bins=np.linspace(-1*fmax[rindex[0]],fmax[rindex[0]],40))\n",
    "plt.title('Support Vector Machine Model Prediction Error - ' + response_feature + ' from ' + predictor_features[0] + ' and ' + predictor_features[1]); \n",
    "plt.xlabel(response_feature + ' Estimation Error '); plt.ylabel('Frequency')\n",
    "plt.xlim(-1*fmax[rindex[0]],fmax[rindex[0]])\n",
    "\n",
    "plt.subplots_adjust(left=0.0, bottom=0.0, right=3.2, top=1.6, wspace=0.1, hspace=0.2)\n",
    "plt.show()\n",
    "\n",
    "Let's put all our models together for one visualization of our:\n",
    "\n",
    "* models with the withheld testing data\n",
    "\n",
    "* error at withheld testing data samples distribution\n",
    "\n",
    "plt.subplot(231)\n",
    "plt = visualize_model(multilinear_reg,X_test[predictor_features[0]],X_test[predictor_features[1]],y_test[response_feature],'Training Data and Multilinear Regression Model')\n",
    "\n",
    "plt.subplot(234)\n",
    "plt.hist(multilinear_y_res, alpha = 0.2, color = 'red', edgecolor = 'black', bins=np.linspace(-1*fmax[rindex[0]],fmax[rindex[0]],40))\n",
    "plt.title('Linear Regression Model Prediction Error - ' + response_feature + ' from ' + predictor_features[0] + ' and ' + predictor_features[1]); \n",
    "plt.xlabel(response_feature + ' Estimation Error '); plt.ylabel('Frequency')\n",
    "plt.xlim(-1*fmax[rindex[0]],fmax[rindex[0]])\n",
    "\n",
    "plt.subplot(232)\n",
    "plt = visualize_model(decision_tree_reg,X_test[predictor_features[0]],X_test[predictor_features[1]],y_test[response_feature],'Training Data and Decision Tree Model')\n",
    "\n",
    "plt.subplot(235)\n",
    "plt.hist(tree_y_res, alpha = 0.2, color = 'red', edgecolor = 'black', bins=np.linspace(-1*fmax[rindex[0]],fmax[rindex[0]],40))\n",
    "plt.title('Decision Tree Model Prediction Error - ' + response_feature + ' from ' + predictor_features[0] + ' and ' + predictor_features[1]); \n",
    "plt.xlabel(response_feature + ' Estimation Error '); plt.ylabel('Frequency')\n",
    "plt.xlim(-1*fmax[rindex[0]],fmax[rindex[0]])\n",
    "\n",
    "plt.subplot(233)\n",
    "plt = visualize_model(support_vector_reg,X_test[predictor_features[0]],X_test[predictor_features[1]],y_test[response_feature],'Training Data and Support Vector Model')\n",
    "\n",
    "plt.subplot(236)\n",
    "plt.hist(svm_y_res, alpha = 0.2, color = 'red', edgecolor = 'black', bins=np.linspace(-1*fmax[rindex[0]],fmax[rindex[0]],40))\n",
    "plt.title('Support Vector Machine Model Prediction Error - ' + response_feature + ' from ' + predictor_features[0] + ' and ' + predictor_features[1]); \n",
    "plt.xlabel(response_feature + ' Estimation Error '); plt.ylabel('Frequency')\n",
    "plt.xlim(-1*fmax[rindex[0]],fmax[rindex[0]])\n",
    "\n",
    "plt.subplots_adjust(left=0.0, bottom=0.0, right=3.2, top=2.6, wspace=0.1, hspace=0.2)\n",
    "plt.show()\n",
    "\n",
    "I'll end here for brevity, but I invite you to continue. There are many other scikit learn methods to explore and there are further opportunities for model cross validation and hyperparameter tuning.  \n",
    "\n",
    "* return to the beginning and select a different pair of predictor features and try again\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Replace 'your_data.csv' with the path to your dataset\n",
    "file_path = 'Cleaned_HackathonData2024.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Separating numeric and non-numeric columns\n",
    "numeric_data = data.select_dtypes(include=['float64', 'int64'])\n",
    "non_numeric_data = data.select_dtypes(exclude=['float64', 'int64'])\n",
    "\n",
    "# Imputers for different types of data\n",
    "numeric_imputer = SimpleImputer(strategy='mean')\n",
    "non_numeric_imputer = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "# Imputation\n",
    "numeric_data_imputed = pd.DataFrame(numeric_imputer.fit_transform(numeric_data), columns=numeric_data.columns)\n",
    "non_numeric_data_imputed = pd.DataFrame(non_numeric_imputer.fit_transform(non_numeric_data), columns=non_numeric_data.columns)\n",
    "\n",
    "# Combine the data back\n",
    "data_imputed_combined = pd.concat([numeric_data_imputed, non_numeric_data_imputed], axis=1)\n",
    "\n",
    "# Basic statistical analysis on the imputed numeric data\n",
    "statistical_analysis_numeric = numeric_data_imputed.describe()\n",
    "statistical_analysis_numeric\n",
    "\n",
    "# Visualization of the completeness of the dataset before and after imputation\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Before imputation\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.heatmap(data.isna(), cbar=False)\n",
    "plt.title('Before Imputation')\n",
    "plt.xlabel('Columns')\n",
    "plt.ylabel('Index')\n",
    "\n",
    "# After imputation\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.heatmap(data_imputed_combined.isna(), cbar=False)\n",
    "plt.title('After Imputation')\n",
    "plt.xlabel('Columns')\n",
    "plt.ylabel('Index')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
